scenario,run,domain,benchmark,sequence_negative_log_likelihood,auroc_sequence_negative_log_likelihood,max_token_negative_log_likelihood,auroc_max_token_negative_log_likelihood,predictive_entropy,auroc_predictive_entropy,shannon_entropy,auroc_shannon_entropy,model
gsm,"gsm:model=microsoft_phi-3.5-mini-instruct,stop=none,max_train_instances=0",mathematical_reasoning,0.8266666666666667,0.13945988735432358,0.7242555831265509,4.166395754019417,0.678349875930521,0.04299767373302521,0.6935483870967742,0.13764336355476822,0.7202233250620347,microsoft_phi-3.5-mini-instruct
math,math,mathematical_reasoning,0.8604651162790697,0.15977932117199997,0.4009009009009009,4.297344462816105,0.5090090090090089,0.04571213665223915,0.40090090090090086,0.1518116853491302,0.38738738738738737,microsoft_phi-3.5-mini-instruct
mmlu,"mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=microsoft_phi-3.5-mini-instruct,max_train_instances=0",knowledge_qa,0.5945945945945946,0.4510881261490293,0.6255892255892256,7.2072832154798085,0.565993265993266,0.09493603136433333,0.576094276094276,0.44904794563187356,0.6191919191919192,microsoft_phi-3.5-mini-instruct
mmlu,"mmlu:subject=college_chemistry,method=multiple_choice_joint,model=microsoft_phi-3.5-mini-instruct,max_train_instances=0",knowledge_qa,0.5462962962962963,0.4694443703082366,0.5482531995849187,6.745846732898994,0.5672777585610516,0.09652894272650217,0.5268073331027326,0.4666110094678703,0.5579384296091318,microsoft_phi-3.5-mini-instruct
mmlu,"mmlu:subject=computer_security,method=multiple_choice_joint,model=microsoft_phi-3.5-mini-instruct,max_train_instances=0",knowledge_qa,0.7477477477477478,0.6191809709609509,0.5731497418244407,6.7943465602290525,0.4079173838209983,0.12270720518611639,0.5387263339070568,0.6264664688805295,0.6234939759036144,microsoft_phi-3.5-mini-instruct
mmlu,"mmlu:subject=econometrics,method=multiple_choice_joint,model=microsoft_phi-3.5-mini-instruct,max_train_instances=0",knowledge_qa,0.5396825396825397,0.6036270928641335,0.5783468559837728,7.411969784706356,0.5306795131845842,0.11494453283148327,0.5476673427991886,0.5953264066652022,0.5907707910750506,microsoft_phi-3.5-mini-instruct
mmlu,"mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=microsoft_phi-3.5-mini-instruct,max_train_instances=0",knowledge_qa,0.7837837837837838,0.6365128047018398,0.5933908045977011,7.005239503877656,0.5804597701149425,0.11893529560207586,0.5636973180076629,0.6338965271954577,0.6020114942528736,microsoft_phi-3.5-mini-instruct
narrative_qa,"narrative_qa:model=microsoft_phi-3.5-mini-instruct,max_train_instances=0",knowledge_qa,0.9133333333333333,0.1992908175770528,0.7434025828186412,1.069498177932285,0.711959573273442,0.0521594578744167,0.7653003930376193,0.18800483768085766,0.7731611454239191,microsoft_phi-3.5-mini-instruct
legalbench,"legalbench:subset=abercrombie,model=microsoft_phi-3.5-mini-instruct,max_train_instances=0",specialized_domains,0.4842105263157895,0.47905712332843847,0.536379769299024,6.759548440732452,0.5554569653948536,0.10464115511767949,0.5124223602484472,0.47347962311210984,0.5350488021295474,microsoft_phi-3.5-mini-instruct
legalbench,"legalbench:subset=corporate_lobbying,model=microsoft_phi-3.5-mini-instruct,max_train_instances=0",specialized_domains,0.4866666666666667,0.7254665474789184,0.6144814090019569,7.0031110978126545,0.5043586550435866,0.12564969347482188,0.6167941647393702,0.7198933930848553,0.6265789005515033,microsoft_phi-3.5-mini-instruct
legalbench,"legalbench:subset=function_of_decision_section,model=microsoft_phi-3.5-mini-instruct,max_train_instances=0",specialized_domains,0.2733333333333333,0.6527781382761012,0.5117475945401656,7.151761975288392,0.5171179234728127,0.12502374291833349,0.5253971805773103,0.645720917346365,0.49384649809800846,microsoft_phi-3.5-mini-instruct
legalbench,"legalbench:subset=international_citizenship_questions,model=microsoft_phi-3.5-mini-instruct,max_train_instances=0",specialized_domains,0.4866666666666667,0.7568686883957931,0.6068315246397438,7.648020273844398,0.5901085216153709,0.1294783757748138,0.6570005337128625,0.7561278212814687,0.6363636363636364,microsoft_phi-3.5-mini-instruct
legalbench,"legalbench:subset=proa,model=microsoft_phi-3.5-mini-instruct,max_train_instances=0",specialized_domains,0.8736842105263158,0.5945981750381526,0.6325301204819277,6.806964116347463,0.33032128514056225,0.1171836529993373,0.6495983935742972,0.5883159284735703,0.608433734939759,microsoft_phi-3.5-mini-instruct
med_qa,"med_qa:model=microsoft_phi-3.5-mini-instruct,max_train_instances=0",specialized_domains,0.6066666666666667,0.566464912002077,0.5412553548146769,7.172864135106404,0.49022164276401564,0.10978093902465012,0.5356677221084002,0.5689518485463633,0.5498230582976346,microsoft_phi-3.5-mini-instruct
