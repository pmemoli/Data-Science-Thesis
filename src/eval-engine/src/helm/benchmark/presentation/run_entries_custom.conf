# HELM scenarios.

entries: [
  # NarrativeQA
  {description: "narrative_qa:model=text,output_format_instructions=narrative_qa,increase_max_tokens=1000,max_train_instances=zero", priority: 1}

  # NaturalQuestions
  {description: "natural_qa:model=text,mode=closedbook,output_format_instructions=natural_qa,increase_max_tokens=1000,max_train_instances=zero", priority: 1}

  # OpenbookQA
  {description: "natural_qa:model=text,mode=closedbook,follow_format_instructions=instruct,max_train_instances=zero", priority: 1}

  # MMLU
  {description: "mmlu:model=text,subject=abstract_algebra,output_format_instructions=mmlu,increase_max_tokens=1000,max_train_instances=zero", priority: 2}
  {description: "mmlu:model=text,subject=college_chemistry,output_format_instructions=mmlu,increase_max_tokens=1000,max_train_instances=zero", priority: 2}
  {description: "mmlu:model=text,subject=computer_security,output_format_instructions=mmlu,increase_max_tokens=1000,max_train_instances=zero", priority: 2}
  {description: "mmlu:model=text,subject=econometrics,output_format_instructions=mmlu,increase_max_tokens=1000,max_train_instances=zero", priority: 2}
  {description: "mmlu:model=text,subject=us_foreign_policy,output_format_instructions=mmlu,increase_max_tokens=1000,max_train_instances=zero", priority: 2}

  # MATH
  {description: "math:model=text_code,subject=number_theory,level=1,use_chain_of_thought=True,increase_max_tokens=1000,max_train_instances=zero", priority: 2}
  {description: "math:model=text_code,subject=intermediate_algebra,level=1,use_chain_of_thought=True,increase_max_tokens=1000,max_train_instances=zero", priority: 2}
  {description: "math:model=text_code,subject=algebra,level=1,use_chain_of_thought=True,increase_max_tokens=1000,max_train_instances=zero", priority: 2}
  {description: "math:model=text_code,subject=prealgebra,level=1,use_chain_of_thought=True,increase_max_tokens=1000,max_train_instances=zero", priority: 2}
  {description: "math:model=text_code,subject=geometry,level=1,use_chain_of_thought=True,increase_max_tokens=1000,max_train_instances=zero", priority: 2}
  {description: "math:model=text_code,subject=counting_and_probability,level=1,use_chain_of_thought=True,increase_max_tokens=1000,max_train_instances=zero", priority: 2}
  {description: "math:model=text_code,subject=precalculus,level=1,use_chain_of_thought=True,increase_max_tokens=1000,max_train_instances=zero", priority: 2}

  # GSM
  {description: "gsm:model=text_code,stop=none,increase_max_tokens=1000,max_train_instances=zero", priority: 2}

  # LegalBench
  {description: "legalbench:model=text_code,subset=abercrombie,output_format_instructions=legalbench,increase_max_tokens=1000,max_train_instances=zero", priority: 2}
  {description: "legalbench:model=text_code,subset=corporate_lobbying,output_format_instructions=legalbench,increase_max_tokens=1000,max_train_instances=zero", priority: 2}
  {description: "legalbench:model=text_code,subset=international_citizenship_questions,output_format_instructions=legalbench,increase_max_tokens=1000,max_train_instances=zero", priority: 2}
  {description: "legalbench:model=text_code,subset=function_of_decision_section,output_format_instructions=legalbench,increase_max_tokens=1000,max_train_instances=zero", priority: 2}
  {description: "legalbench:model=text_code,subset=proa,output_format_instructions=legalbench,increase_max_tokens=1000,max_train_instances=zero", priority: 2}

  # MedQA
  {description: "med_qa:model=text_code,output_format_instructions=med_qa,increase_max_tokens=1000,max_train_instances=zero", priority: 2}
]

